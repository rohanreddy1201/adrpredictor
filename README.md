# ðŸ§  ADRPredictor â€“ Adverse Drug Reaction Risk Analyzer

ADRPredictor is a lightweight, real-time app that predicts the likelihood of serious adverse drug reactions (ADRs) based on patient data and drug characteristics â€” built using real FAERS pharmacovigilance data and powered by local LLM explanations.

![Screenshot](app/assets/screenshot.png) <!-- Add screenshot later -->

## âš¡ Features

- Predicts **serious ADR risk** from drug, age, sex, route, indication, and dosage form
- Uses a trained machine learning model (RandomForest) on a flattened FAERS dataset
- Provides **natural language risk explanations** powered by local LLM (via Ollama + Mistral)
- Runs completely offline â€” no cloud APIs, no keys
- Clean and fast UI built with **Streamlit**
- Fully open-source and reproducible

## ðŸ“¦ Tech Stack

- Python, Streamlit
- scikit-learn, SHAP, pandas
- Ollama (for local LLM inference)
- FAERS JSON dataset flattened to CSV for model training

## ðŸš€ How to Run Locally

1. **Clone the repo**  
   ```bash
   git clone https://github.com/rohanreddy1201/adrpredictor.git
   cd adrpredictor
   ```

2. **(Optional) Set up Python**  
   Use `pyenv`, conda, or system Python (`>=3.9` recommended).

3. **Install dependencies**  
   ```bash
   pip install -r requirements.txt
   ```

4. **Start Ollama LLM backend** (for explanations)
   ```bash
   brew install ollama
   ollama serve
   ollama pull mistral
   ```

5. **Run the app**
   ```bash
   streamlit run app/main.py
   ```

> âœ… Your browser will open to `http://localhost:8501`

---

## ðŸ§  Example Input

```text
Drug: Abacavir
Indication: Accidental Exposure
Age: 50
Sex: Male
Route: Infiltration
Dosage Form: Capsule, Delayed Release
```

---

## ðŸ“Š Output

- A **risk score** (0â€“100%) of serious ADRs
- A **generated explanation** using medical language from local LLM
- Feature contributions and SHAP insights

---

## ðŸ§ª Dataset

- Based on real **FAERS structured JSON** drug-event data
- Preprocessed and flattened to `app/data/flattened_faers.csv`

---

## ðŸ§  LLM Explanation

- Powered by [Ollama](https://ollama.com) with the **Mistral** model
- No external API calls, no latency
- Context-aware risk descriptions based on inputs

---

## ðŸ›  Project Structure

```
adrpredictor/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                  # Streamlit app entry
â”‚   â”œâ”€â”€ model/                   # ML model + ollama explanation
â”‚   â”œâ”€â”€ utils/                   # Input config, LLM, helpers
â”‚   â””â”€â”€ data/                    # Processed FAERS dataset
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ“Œ Author

**Rohan Reddy**  
AI Cloud Engineer Â· [LinkedIn](https://www.linkedin.com/in/roreddy/)  
[Portfolio](https://rohanreddy1201.github.io)

---

## ðŸ§© License

MIT License